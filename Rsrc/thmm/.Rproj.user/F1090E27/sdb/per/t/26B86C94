{
    "collab_server" : "",
    "contents" : "# -------------------------------------------------\n# $Id: $\n# tinyhmm : package entry (documentation and attachment)\n\n# -------------------------------------------------\n#' Overview of package thmm\n#' @name thmm\n#' @description\n#' A tiny R package for discrete time hidden Markov models (DTHmm)\n#' with careful handling of underflow.\\cr\n#' Strongly inspired by the 'HiddenMarkov' package from David Harte (\\url{https://cran.r-project.org/web/packages/HiddenMarkov/index.html})\n#' but numerically more stable (for forward/backward computations) and with different (hopefully simpler) API.\n#' (the 'HiddenMarkov' package has more features (including Markov modulated GLMs))\n#' \\subsection{Concepts}{\n#' A discrete time hidden Markov model is defined by:\\cr\n#' \\itemize{\n#' \\item m (discrete) states\n#' \\item a probability density function \\code{density} describing the (univariate\n#' or multivariate, continuous or discrete) distribution with different parameters\n#' for each state.\n#' \\item a transition probability matrix (of size m x m) between states\n#' \\item a vector (of length m) of initial probability for each state\n#' }\n#' \\bold{important note}: for the sake of simplicty the remaining of this\n#' documentation as well as functions documentation, will assume that\n#' \\code{density} is univariate. However the library can be used with multivariate\n#' densities as well (of course this comes at a price of a bit of complication).\n#' Please consult the \\bold{multivariate densities} section below.\n#'\n#' Given a sequence of observed (univariate) random variable \\code{X}\n#' noted \\code{obs=obs_1, ..., obs_n} this package solves the following problems:\\cr\n#' \\enumerate{\n#' \\item Given the model parameters, compute the probability of \\code{obs}.\n#' This problem is solved by the \\bold{forward and backward} algorithms.\n#' \\item Given the model parameters, find the most likely sequence of (hidden) states\n#' which could have generated \\code{obs}. This problem is solved by the \\bold{Viterbi}\n#' algorithm.\n#' \\item Given \\code{obs}, find the most likely set of \\code{density} parameters and\n#' transition probabilities. This problem is solved by the \\bold{Baum-Welch} algorithm.\n#' }\n#' }\n#' \\subsection{forward algorithm}{\n#' is used to compute:\n#' \\itemize{\n#' \\item\\code{alpha_{i,j} = Pr{ X_1 = obs_1, ..., X_i = obs_i, state_i = j | hmm}}\n#' that is the probability of seeing the partial sequence\n#' \\code{(obs_1, ..., obs_i)} and ending up in state j at time i for this\n#' hmm.\n#' }\n#' from which one can derive the \\bold{log-likelihood} of observation \\code{obs} with this hmm:\n#' \\itemize{\n#' \\item\\code{log(Pr{ X_1 = obs_1, ..., X_n = obs_n | hmm})}\n#' }\n#' }\n#' \\subsection{backward algorithm}{\n#' is used to compute:\n#' \\itemize{\n#' \\item\\code{beta_{i,j} = Pr{ X_{i+1} = obs_{i+1}, ..., X_n = obs_n | state_i = j , hmm}}\n#' that is the probability of the ending partial sequence\n#' \\code{(obs_i+1, ..., obs_n)} given that we started at state j at time i,\n#' for this hmm\n#' }\n#' }\n#' \\subsection{forward-backward algorithm}{\n#' putting everything together, one can further compute:\n#' \\itemize{\n#' \\item\\code{gamma_{i,j} = Pr{ state_i = j | obs , hmm}}\n#' that is the probability of being at time i in state j given\n#' this observation and this hmm\n#' \\item\\code{rho_{i} = Pr{ X_1 = obs_1, ..., X_i = obs_i | hmm}}\n#' that is the probability of seeing the partial sequence\n#' \\code{(obs_1, ..., obs_i)} with this hmm\n#' }\n#' }\n#' \\subsection{Viterbi's algorithm}{\n#' The purpose of the Viterbi's algorithm is to determine\n#' the sequence of states \\code{(k_1*, ..., k_n*)} which maximises\n#' the joint distribution of the hidden states given the entire\n#' observed process. i.e:\n#' \\itemize{\n#' \\item\\code{(k_1*, ..., k_n*) = argmax(Pr(state_1=k_1, ..., state_n=k_n, X_1=obs_1, ..., X_n=obs_n | hmm))}\n#' }\n#' this also allows to compute:\n#' \\itemize{\n#' \\item\\code{nu_{i+1,j} = Pr(state_1=k_1*, ..., state_i=k_i*, state_i+1 = j,  X_1=obs_1, ..., X_i=obs_i | hmm)}\n#' where k_i* is the optimal state at time i. (local decoding)\n#' \\item\\code{viterbi_score = Pr(state_1=k_1*, ..., state_n=k_n*, obs | hmm)} the joint\n#' probability of optimal sequence of states and observation.\n#' \\item\\code{probseq  = Pr(state_1=k_1*, ..., state_n=k_n* | obs, hmm)} the probability\n#' of the optimal sequence of state conditionally to the observation.\n#' }\n#' }\n#' \\subsection{Baum-Welch algorithm}{\n#' The purpose of the Baum-Welch algorithm is to estimate the HMM parameters\n#' (i.e. distribution parameters and transition probabilities) that\n#' best fit a given observation. This is a version of the EM algorithm that\n#' iteratively alternates two steps: the \\bold{E-step}\n#' and the \\bold{M-step}. The E-step is generic but the M-step depends upon\n#' each distribution. A M-step for Normal distribution is provided\n#' (as \\code{mstep.dnorm}) and some others are given in examples.\n#' See \\link{thmm.baumwelch} for more details (in particular how to\n#' control which parameters are actually adjusted).\n#' }\n#' \\subsection{Implementation}{\n#' current implementation uses either pure R code or compiled C code.\n#' the default is to use (quicker) C code, R code has been kept for\n#' debugging purposes.\n#' }\n#' \\subsection{Multivariate densities}{\n#' Instead of a univariate density function you may define a bi- or multi-\n#' variate density function as well.\n#' The main difference is that instead of a numerical vector of observations,\n#' you should provide a list of tuples (each element in the tuple corresponds\n#' to one of the random variable). For the bivariate case, however, a trick is\n#' to use complex numbers instead of a list of doublets. This makes the definition\n#' of density and observation much simpler. Please see examples\n#' below as well as samples in the \\code{test} directory.\n#' }\n#' \\subsection{Serializing}{\n#' \\bold{Important note} In the current implementation a \\code{DTHmm} object\n#' keeps internally tracks of the \\link{parent.frame} environment where\n#' \\link{thmm.init} has been called. This is necessary to ensure that the\n#' \\code{...} arguments will be interpreted in the correct context.\n#' A drawback arise when you want to serialize (thru \\link{serialize}, \\link{save}\n#' or \\link{saveRDS}) this object. R will also serialize this environment and\n#' this may gives rise to a potentially huge file (if you have large objects\n#' in this environment).\\cr\n#' For now, a workaround is to replace this environment by \\link{globalenv}.\n#' This is safe if you do not refer to any parent frame variable in the\n#' code of \\code{density} (which is a good programming\n#' practice). A (temporary) helper function \\link{thmm.forget.env} is provided\n#' to perform this replacement. You may call it just before serializing.\\cr\n#' This unfortunate behavior will be changed in the future. I just need a bit\n#' of time to think about possible consequences of forgetting parent frame.\n#' }\n#'\n#' @examples\n#' # ---------------------------------------\n#' # Normal (gaussian) HMM with two states\n#' # state 1 : mean=-1, sd=0.1\n#' # state 2 : mean=+1, sd=0.1\n#' # ergodic HMM with transition probability = 0.1\n#' #\n#' # full specification:\n#' trans <- matrix(c(0.9, 0.1, 0.1, 0.9), nrow=2)\n#' init <- c(0.5, 0.5)\n#' hmm <- thmm.init(dnorm, trans, init, mean=c(-1,1), sd=c(0.1, 0.1))\n#' #\n#' # *same as previous* with simplified call:\n#' hmm <- thmm.init(dnorm, 0.1, mean=c(-1,1), sd=0.1)\n#'\n#' # simulate some sample\n#' obs <- thmm.simulate(hmm, 100, .seed=0)\n#'\n#' # run viterbi to retrieve states\n#' vit <- thmm.viterbi(hmm, obs$values)\n#'\n#' \\dontrun{\n#' # compare actual and predicted states\n#' plot(obs$values)\n#' lines(thmm.parameters(hmm, \"mean\")[obs$states], col=3, lwd=5)\n#' lines(thmm.parameters(hmm, \"mean\")[vit$states], col=2)}\n#'\n#' # ---------------------------------------\n#' # poisson distribution\n#' #\n#' hmm <- thmm.init(dpois, 0.1, lambda=c(5, 10))\n#' obs <- thmm.simulate(hmm, 100, .seed=0)\n#' vit <- thmm.viterbi(hmm, obs$values)\n#' \\dontrun{\n#' plot(obs$values)\n#' lines(thmm.parameters(hmm, \"lambda\")[obs$states], col=3, lwd=5)\n#' lines(thmm.parameters(hmm, \"lambda\")[vit$states], col=2)}\n#'\n#' # ---------------------------------------\n#' # mixture of gaussians\n#' #\n#' # we need to define the pdf\n#' #\n#' # note that this function is 'vectorized' on parameters (and x).\n#' # the computation with log=TRUE will avoid underflow\n#' #\n#' dmixnorm <- function(x, mean=0, sd=1, prob=1, log=FALSE) {\n#'   msp <- mapply(function(m, s, p) list(mean=m, sd=s, prob=p), mean, sd, prob, SIMPLIFY=FALSE)\n#'   res <- if (log) {\n#'     lapply(x, function(x) sapply(msp, function(p) {\n#'       pp <- base::log(p$prob) + dnorm(x, mean=p$mean, sd=p$sd, log=TRUE)\n#'       mp <- max(pp)\n#'       mp + base::log(sum(exp(pp-mp)))\n#'     }))\n#'   } else {\n#'     lapply(x, function(x) sapply(msp, function(p)\n#'       sum(p$prob*dnorm(x, mean=p$mean, sd=p$sd, log=FALSE))))\n#'   }\n#'   unlist(res)\n#' }\n#' #\n#' # and random generator\n#' # again 'vectorized' on parameters (and x).\n#' rmixnorm <- function(n, mean=0, sd=1, prob=1) {\n#'   .cycle <- function(x, n) head(rep(x, n), n)\n#'   msp <- mapply(function(m, s, p) list(mean=m, sd=s, prob=p), mean, sd, prob, SIMPLIFY=FALSE)\n#'   unlist(lapply(seq_len(n), function(i) sapply(msp, function(p) {\n#'          r <- mapply(function(m, s) rnorm(1, m, s), p$mean, p$sd)\n#'          sample(r, size=1, prob=.cycle(p$prob, length(r)))\n#'   })))\n#' }\n#'\n#' # hmm with 2 states\n#' # state1: mixture of 2 gaussians with mean=-1,1 sd=0.1,0.1 and prob=0.1,0.9\n#' # state2: single gaussian mean=0 sd=0.2\n#' hmm <- thmm.init(dmixnorm, 0.2, mean=list(c(-1,1), 0), sd=list(0.1, 0.2),\n#'                                 prob=list(c(0.5,0.5), 1))\n#' obs <- thmm.simulate(hmm, n=100, .seed=0)\n#' vit <- thmm.viterbi(hmm, obs$values)\n#' \\dontrun{\n#' plot(obs$values)\n#' lines((2-obs$states), col=3)\n#' y <- thmm.parameters(hmm, \"mean\")[vit$states]\n#' lines(sapply(y, function(x) x[2%%length(x)+1]), col=2)}\n#'\n#' # ---------------------------------------\n#' # discrete distribution : the dishonest casino\n#' #\n#' # the pdf (with p6 parameter = probability of drawing a six)\n#' dice <- function(x, p6=1/6, log=FALSE) {\n#'   r <- unlist(lapply(x, function(x) {\n#'     unlist(lapply(p6, function(p6) {\n#'       if (is.na(x)) NA\n#'       else if (x == 6) p6\n#'       else if (x %in% 1:5) (1-p6)/5\n#'       else 0\n#'     })) }), recursive=FALSE)\n#'   if (log) r <- base::log(r)\n#'   r\n#' }\n#' # and the random generator (d->r)ice\n#' rice <- function(n, p6=1/6) {\n#'   sample(1:6, n, prob=c(rep((1-p6)/5, 5), p6), replace=TRUE)\n#' }\n#'\n#' hmm <- thmm.init(dice, 0.1, p6=c(1/6, 3/6))  # the second dice is loaded\n#' obs <- thmm.simulate(hmm, 100, .seed=0)\n#' vit <- thmm.viterbi(hmm, obs$values)\n#' \\dontrun{\n#' plot(obs$values)\n#' lines((obs$states-1)*5+1, col=3) # truth\n#' lines((vit$states-1)*5+1, col=2) #predicted}\n#'\n#' # ---------------------------------------\n#' # univariate normal : Baum-Welch\n#' #\n#' # reference hmm\n#' hmm <- thmm.init(dnorm, 0.1, mean=c(-1,1), sd=0.1)\n#' obs <- thmm.simulate(hmm, n=1000, .seed=0)\n#'\n#' # this one converge to the correct solution\n#' hmm0 <- thmm.init(dnorm, 0.1, mean=c(0,5))\n#' bw   <- thmm.baumwelch(hmm0, obs$values)\n#'\n#' # but not that one\n#' hmm0 <- thmm.init(dnorm, 0.1, mean=c(0,10))\n#' bw   <- thmm.baumwelch(hmm0, obs$values)\n#'\n#' # so adding constraints: no update of trans nor init\n#' hmm0 <- thmm.init(dnorm, 0.1, mean=c(0,10))\n#' ctrl <- thmm.bw.ctrl(hmm0, do.trans=FALSE, do.init=FALSE)\n#' bw   <- thmm.baumwelch(hmm0, obs$values, ctrl)\n#'\n#'\n#' # ---------------------------------------\n#' # bivariate normal\n#' # implemented using complex numbers\n#' # see test.multivar.norm.r for a more general multivariate version\n#' #\n#'\n#' # helper\n#' .c <- function(x,y=x) complex(real=x, imaginary=y)\n#'\n#' #\n#' # pdf bivariate normal (with 0 covariance)\n#' #\n#' dxynorm <- function(x, mean=.c(0), sd=.c(1), log=FALSE) {\n#'   re <- dnorm(Re(x), mean=Re(mean), sd=Re(sd), log=log)\n#'   im <- dnorm(Im(x), mean=Im(mean), sd=Im(sd), log=log)\n#'   if (log) re+im else re*im\n#' }\n#'\n#' #\n#' # random generation function\n#' #\n#' rxynorm <- function(n, mean=.c(0), sd=.c(1)) {\n#'   re <- rnorm(n, mean=Re(mean), sd=Re(sd))\n#'   im <- rnorm(n, mean=Im(mean), sd=Im(sd))\n#'   .c(re, im)\n#' }\n#'\n#' hmm <- thmm.init(dxynorm, 0.1,\n#'                  mean=c(.c(10, 20), .c(50, 100)),\n#'                  sd=c(.c(5,10),.c(25, 40)))\n#'\n#' obs <- thmm.simulate(hmm, 100, with.values=TRUE)\n#' vit <- thmm.viterbi(hmm, obs$values)\n#' lvl <- thmm.parameters(hmm, \"mean\")\n#'\n#' \\dontrun{\n#' par(mfrow=c(2,1))\n#' plot(Re(obs$values))\n#' lines(Re(lvl[obs$states]), col=3, lwd=2)\n#' lines(Re(lvl[vit$states]), col=2, lty=2, lwd=2)\n#' plot(Im(obs$values))\n#' lines(Im(lvl[obs$states]), col=3, lwd=2)\n#' lines(Im(lvl[vit$states]), col=2, lty=2, lwd=2)\n#' par(mfrow=c(1,1))}\n#'\n#' # ---------------------------------------\n#' # bivariate normal + poisson\n#' #\n#'\n#' # helper : take ith element of tuple\n#' .i <- function(x, i) sapply(x, function(x) x[i])\n#'\n#' # pdf : bivariate normal + poisson\n#' #\n#'\n#' dnorpois <- function(x, mean=0, sd=1, lambda=10, log=FALSE) {\n#'   .prod <- if (log) sum else prod\n#'   unlist(mapply(function(m, s, l) {\n#'     xn <- dnorm(.i(x,1), m, s, log=log)\n#'     xp <- dpois(.i(x,2), l, log=log)\n#'     mapply(.prod, xn, xp)\n#'   },\n#'   mean, sd, lambda, SIMPLIFY=FALSE), use.names=FALSE)\n#' }\n#'\n#' # random number generator\n#' #\n#'\n#' rnorpois <- function(n, mean=0, sd=1, lambda=10) {\n#'   mapply(c, rnorm(n, mean, sd), rpois(n, lambda), SIMPLIFY=FALSE)\n#' }\n#'\n#' hmm <- thmm.init(dnorpois, 0.1,\n#'                  mean=c(10, 20, 30), sd=c(1, 2, 3),\n#'                  lambda=c(5, 10, 15))\n#'\n#' obs <- thmm.simulate(hmm, 100, with.values=TRUE)\n#'\n#' vit <- thmm.viterbi(hmm, obs$values)\n#'\n#' lvl.mean <- thmm.parameters(hmm, \"mean\")\n#' lvl.lamb <- thmm.parameters(hmm, \"lambda\")\n#'\n#' \\dontrun{\n#' par(mfrow=c(2,1))\n#' plot(.i(obs$values,1), main=\"normal\")\n#' lines(lvl.mean[obs$states], col=3, lwd=2)\n#' lines(lvl.mean[vit$states], col=2, lty=2, lwd=2)\n#' plot(.i(obs$values,2), main=\"poisson\")\n#' lines(lvl.lamb[obs$states], col=3, lwd=2)\n#' lines(lvl.lamb[vit$states], col=2, lty=2, lwd=2)\n#' par(mfrow=c(1,1))}\n#'\n#'\nNULL\n\n# =================================================\n# package attachment\n# =================================================\n#\n\n.onAttach <- function(libname, pkgname) {\n  packageStartupMessage('+ attaching ', pkgname)\n}\n",
    "created" : 1499189195537.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4197759504",
    "id" : "26B86C94",
    "lastKnownWriteTime" : 1502891833,
    "last_content_update" : 1502891833549,
    "path" : "~/Developpements/SvnProjects/WGInR_Asdog/Rsrc/thmm/R/thmm.r",
    "project_path" : "R/thmm.r",
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}